# ğŸ“š Resumen TeÃ³rico - Tarea 2

## PARTE 1: FILTROS DE IMÃGENES (30%)

### 1. Filtro de Media

**DefiniciÃ³n**: Promedio aritmÃ©tico de pÃ­xeles vecinos en una ventana.

**FÃ³rmula**:
```
g(x,y) = (1/mn) Ã— Î£ Î£ f(x+i, y+j)
```
donde `mÃ—n` es el tamaÃ±o de la mÃ¡scara.

**Ejemplo**: Ventana 5Ã—5 â†’ promedia 25 pÃ­xeles

**Ventajas**:
- âœ… Simple y eficiente
- âœ… Reduce ruido gaussiano
- âœ… Preserva la media global

**Desventajas**:
- âŒ Difumina bordes
- âŒ Sensible a valores atÃ­picos

---

### 2. Filtro de Mediana

**DefiniciÃ³n**: Reemplaza cada pÃ­xel por la mediana de su vecindad.

**FÃ³rmula**:
```
g(x,y) = mediana{f(x+i, y+j) : (i,j) âˆˆ W}
```

**Ejemplo**: Kernel 3Ã—3 â†’ ordena 9 valores, toma el del medio

**Ventajas**:
- âœ… Excelente para ruido sal y pimienta
- âœ… Preserva bordes mejor que la media

**Desventajas**:
- âŒ MÃ¡s costoso computacionalmente
- âŒ Puede distorsionar texturas finas

---

### 3. Filtro LogarÃ­tmico

**DefiniciÃ³n**: TransformaciÃ³n punto a punto que comprime rango dinÃ¡mico.

**FÃ³rmula**:
```
g(x,y) = c Ã— log(1 + f(x,y))
```
donde `c` es constante de escala.

**Ejemplo**: `c = 255/log(2)` para imÃ¡genes 8-bit

**Ventajas**:
- âœ… Realza detalles en sombras
- âœ… Ãštil para HDR (High Dynamic Range)

**Desventajas**:
- âŒ Amplifica ruido en bajas intensidades
- âŒ Requiere normalizaciÃ³n

---

### 4. Filtro de Cuadro Normalizado

**DefiniciÃ³n**: Variante del filtro de media con coeficientes uniformes que suman 1.

**FÃ³rmula**:
```
g(x,y) = Î£ Î£ (1/kÂ²) Ã— f(x+i, y+j)
```
donde `k` es el lado del kernel cuadrado.

**Ejemplo**: Kernel 7Ã—7 â†’ cada coeficiente = 1/49

**Ventajas**:
- âœ… Suavizado controlado
- âœ… ImplementaciÃ³n optimizada (integrales)

**Desventajas**:
- âŒ Similar a media: difumina bordes
- âŒ Artefactos de bloque si se aplica iterativamente

---

### 5. Filtro Gaussiano

**DefiniciÃ³n**: ConvoluciÃ³n con funciÃ³n gaussiana 2D.

**FÃ³rmula**:
```
G(i,j) = (1/(2Ï€ÏƒÂ²)) Ã— exp(-(iÂ² + jÂ²)/(2ÏƒÂ²))
```

**Ejemplo**: Ïƒ=1.0, mÃ¡scara 5Ã—5 â†’ mayor peso al centro

**Ventajas**:
- âœ… Reduce ruido gaussiano eficientemente
- âœ… Separable en 1D (mÃ¡s rÃ¡pido)
- âœ… Preserva bordes mejor que media

**Desventajas**:
- âŒ Difumina detalles muy finos
- âŒ Requiere elegir Ïƒ apropiado

---

### 6. Filtro Laplace

**DefiniciÃ³n**: Derivada de segundo orden, detecta cambios bruscos de intensidad.

**FÃ³rmula**:
```
âˆ‡Â²f = âˆ‚Â²f/âˆ‚xÂ² + âˆ‚Â²f/âˆ‚yÂ²

AproximaciÃ³n discreta:
âˆ‡Â²f â‰ˆ -4f(x,y) + f(x+1,y) + f(x-1,y) + f(x,y+1) + f(x,y-1)
```

**Kernel comÃºn**:
```
[ 0  1  0]
[ 1 -4  1]
[ 0  1  0]
```

**Ventajas**:
- âœ… Detecta bordes en todas direcciones
- âœ… ImplementaciÃ³n simple

**Desventajas**:
- âŒ Muy sensible al ruido
- âŒ Resultado no directamente visualizable

---

### 7. Filtro Sobel

**DefiniciÃ³n**: Derivada de primer orden con suavizado, calcula gradiente direccional.

**FÃ³rmulas**:
```
Gx = [-1  0  1]      Gy = [-1 -2 -1]
     [-2  0  2]           [ 0  0  0]
     [-1  0  1]           [ 1  2  1]

Magnitud: |âˆ‡f| = âˆš(GxÂ² + GyÂ²)
```

**Ventajas**:
- âœ… Detecta bordes con reducciÃ³n de ruido
- âœ… Computacionalmente eficiente
- âœ… Proporciona direcciÃ³n del gradiente

**Desventajas**:
- âŒ Sensible a ruido fuerte
- âŒ Requiere umbralizaciÃ³n posterior

---

### 8. Filtro Canny

**DefiniciÃ³n**: Detector de bordes multietapa optimizado.

**Pasos**:
1. Suavizado gaussiano
2. CÃ¡lculo de gradiente (Sobel)
3. SupresiÃ³n no-mÃ¡xima
4. UmbralizaciÃ³n con histÃ©resis (doble umbral)

**ParÃ¡metros tÃ­picos**: Ïƒ=1, umbral_bajo=50, umbral_alto=150

**Ventajas**:
- âœ… Mejor detector de bordes (localizaciÃ³n precisa)
- âœ… Bordes continuos y delgados
- âœ… Control fino con umbrales

**Desventajas**:
- âŒ MÃ¡s costoso computacionalmente
- âŒ Sensible a elecciÃ³n de parÃ¡metros

---

## PARTE 2: DESCRIPTORES Y CLASIFICACIÃ“N (70%)

### Descriptores de CaracterÃ­sticas

#### HOG (Histogram of Oriented Gradients)

**Concepto**: Histograma de gradientes orientados en regiones locales.

**ParÃ¡metros**:
- Orientaciones: 9 bins (0-180Â°)
- PÃ­xeles por celda: 8Ã—8
- Celdas por bloque: 2Ã—2
- NormalizaciÃ³n: L2-Hys

**Proceso**:
1. Calcular gradientes (magnitud y direcciÃ³n)
2. Dividir imagen en celdas
3. Crear histograma de orientaciones por celda
4. Normalizar bloques de celdas
5. Concatenar vectores

**Aplicaciones**: DetecciÃ³n de peatones, reconocimiento de objetos, OCR

**Ventajas**:
- âœ… Robusto a cambios de iluminaciÃ³n
- âœ… Captura informaciÃ³n de forma y contorno
- âœ… Invariante a pequeÃ±as deformaciones

**Desventajas**:
- âŒ Alto dimensional
- âŒ Sensible a rotaciÃ³n
- âŒ No captura informaciÃ³n de color

---

#### LBP (Local Binary Patterns)

**Concepto**: Descriptor de textura que compara pÃ­xeles con sus vecinos.

**ParÃ¡metros**:
- Radio: 3 pÃ­xeles
- Puntos: 24 (8 Ã— radio)
- MÃ©todo: uniform (patrones uniformes)

**Proceso**:
1. Para cada pÃ­xel, comparar con vecinos circulares
2. Asignar 1 si vecino â‰¥ centro, 0 si <
3. Convertir patrÃ³n binario a nÃºmero decimal
4. Generar histograma de patrones

**FÃ³rmula**:
```
LBP(x,y) = Î£ s(gp - gc) Ã— 2^p

donde s(x) = 1 si x â‰¥ 0, 0 si x < 0
```

**Aplicaciones**: Reconocimiento facial, anÃ¡lisis de texturas, clasificaciÃ³n de materiales

**Ventajas**:
- âœ… Invariante a cambios monÃ³tonos de iluminaciÃ³n
- âœ… Computacionalmente eficiente
- âœ… DimensiÃ³n baja (histograma compacto)

**Desventajas**:
- âŒ Pierde informaciÃ³n de contraste
- âŒ Sensible a rotaciÃ³n (sin extensiones)
- âŒ No captura informaciÃ³n de forma global

---

### Clasificadores

#### SVM (Support Vector Machine)

**Concepto**: Encuentra el hiperplano Ã³ptimo que maximiza el margen entre clases.

**FunciÃ³n objetivo**:
```
min (1/2)||w||Â² + C Ã— Î£ Î¾i

sujeto a: yi(wÂ·xi + b) â‰¥ 1 - Î¾i
```

**Kernel lineal** (usado aquÃ­):
```
K(xi, xj) = xi Â· xj
```

**Ventajas**:
- âœ… Efectivo en espacios de alta dimensiÃ³n
- âœ… Robusto al sobreajuste
- âœ… Funciona bien con datasets pequeÃ±os

**Desventajas**:
- âŒ Costoso para datasets grandes
- âŒ Sensible a desbalance de clases
- âŒ Requiere normalizaciÃ³n de datos

---

#### CNN (Convolutional Neural Network)

**Arquitectura usada**:
```
Input (3Ã—128Ã—64)
    â†“
Conv2d(3â†’32) + BN + ReLU + MaxPool
    â†“
Conv2d(32â†’64) + BN + ReLU + MaxPool
    â†“
Conv2d(64â†’128) + BN + ReLU + MaxPool
    â†“
Conv2d(128â†’256) + BN + ReLU + AdaptiveAvgPool
    â†“
Flatten â†’ Dropout(0.4) â†’ Linear(256â†’128) â†’ ReLU
    â†“
Dropout(0.3) â†’ Linear(128â†’36)
    â†“
Output (36 clases)
```

**Componentes**:
- **ConvoluciÃ³n**: Aprende filtros automÃ¡ticamente
- **BatchNorm**: Estabiliza entrenamiento
- **MaxPool**: Reduce dimensionalidad
- **Dropout**: Previene sobreajuste
- **AdaptiveAvgPool**: Independiente de tamaÃ±o

**OptimizaciÃ³n**:
- FunciÃ³n de pÃ©rdida: CrossEntropyLoss
- Optimizador: Adam
- Learning rate: 0.001
- Weight decay: 0.0001

**Ventajas**:
- âœ… Aprende caracterÃ­sticas automÃ¡ticamente
- âœ… Superior desempeÃ±o general
- âœ… Robusto a variaciones

**Desventajas**:
- âŒ Requiere mÃ¡s datos
- âŒ Computacionalmente intensivo
- âŒ "Caja negra" (difÃ­cil interpretaciÃ³n)

---

### MÃ©tricas de EvaluaciÃ³n

#### 1. Accuracy (Exactitud)
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
ProporciÃ³n de predicciones correctas.

#### 2. Precision (PrecisiÃ³n)
```
Precision = TP / (TP + FP)
```
De las predicciones positivas, cuÃ¡ntas son correctas.

#### 3. Recall (Sensibilidad)
```
Recall = TP / (TP + FN)
```
De los casos positivos reales, cuÃ¡ntos se detectaron.

#### 4. F1-Score
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
Media armÃ³nica entre precision y recall.

#### 5. Matriz de ConfusiÃ³n

```
                Predicho
              Pos    Neg
Real  Pos  [  TP  |  FN  ]
      Neg  [  FP  |  TN  ]
```

- **TP (True Positive)**: Positivos correctamente clasificados
- **TN (True Negative)**: Negativos correctamente clasificados
- **FP (False Positive)**: Negativos clasificados como positivos (Error Tipo I)
- **FN (False Negative)**: Positivos clasificados como negativos (Error Tipo II)

---

## ComparaciÃ³n de Enfoques

| Aspecto | SVM+HOG | SVM+LBP | CNN |
|---------|---------|---------|-----|
| **Accuracy tÃ­pica** | 75-85% | 70-80% | 85-95% |
| **Tiempo entrenamiento** | Medio | Bajo | Alto |
| **Tiempo inferencia** | Bajo | Bajo | Medio |
| **Interpretabilidad** | Alta | Alta | Baja |
| **Requiere ingenierÃ­a** | SÃ­ | SÃ­ | No |
| **Datos necesarios** | Pocos | Pocos | Moderados |
| **GeneralizaciÃ³n** | Media | Media | Alta |

---

## Referencias AcadÃ©micas

1. **Filtros**: Gonzalez & Woods - "Digital Image Processing"
2. **HOG**: Dalal & Triggs (2005) - "Histograms of Oriented Gradients for Human Detection"
3. **LBP**: Ojala et al. (2002) - "Multiresolution Gray-Scale and Rotation Invariant Texture Classification"
4. **SVM**: Cortes & Vapnik (1995) - "Support-Vector Networks"
5. **CNN**: LeCun et al. (1998) - "Gradient-Based Learning Applied to Document Recognition"
6. **Canny**: Canny (1986) - "A Computational Approach to Edge Detection"

---

**Documento preparado para**: Tarea 2 - Filtros y Descriptores de ImÃ¡genes  
**Ãšltima actualizaciÃ³n**: Noviembre 2025
